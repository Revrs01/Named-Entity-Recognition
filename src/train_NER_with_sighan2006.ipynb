{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_conll(filename):\n",
    "    df = pd.read_csv(filename,\n",
    "                    sep = '\\t', header = None, keep_default_na = False,\n",
    "                    names = ['words', 'labels'], skip_blank_lines = False)\n",
    "    df['sentence_id'] = (df.words == '').cumsum()\n",
    "    return df[df.words != '']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T15:16:35.156608Z",
     "end_time": "2023-04-26T15:16:35.183617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = read_conll('../Datasets/web_datasets/sig_train.txt')\n",
    "test_df = read_conll('../Datasets/web_datasets/sig_test.txt')\n",
    "print(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T15:16:35.164770Z",
     "end_time": "2023-04-26T15:16:36.272127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   Train  Test\n0  46364  4365",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train</th>\n      <th>Test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>46364</td>\n      <td>4365</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[train_df['sentence_id'].nunique(), test_df['sentence_id'].nunique()]]\n",
    "pd.DataFrame(data, columns=[\"Train\", \"Test\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T15:16:36.264193Z",
     "end_time": "2023-04-26T15:16:36.292244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'sliding_window': True,\n",
    "    'max_seq_length': 64,\n",
    "    'num_train_epochs': 5,\n",
    "    'train_batch_size': 32,\n",
    "    'fp16': True,\n",
    "    'output_dir': '/outputs/',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T15:16:36.291748Z",
     "end_time": "2023-04-26T15:16:36.303504Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-chinese/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-chinese/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-chinese/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/21 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8a4f23dcaa940fdbd296db52fd24227"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e75ab0d9b9145ee90fda2165a0fbfa1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Running Epoch 0 of 5:   0%|          | 0/1449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f7179d36dd3480abcba9b8554004113"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdf5\\Desktop\\Named-Entity-Recognition\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "Running Epoch 1 of 5:   0%|          | 0/1449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ff6b962d6e3483e8ec83cc35204d437"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Running Epoch 2 of 5:   0%|          | 0/1449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aecd20a377be470793ff672fd8ba8247"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Running Epoch 3 of 5:   0%|          | 0/1449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0720dc7baf04dee90dca0623d8554f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Running Epoch 4 of 5:   0%|          | 0/1449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4f355a2b9c742f4bf07921c1072a011"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model: Training of bert model complete. Saved to /outputs/.\n",
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6c940faec6049d3bb43b8abc37e2e26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Running Evaluation:   0%|          | 0/546 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1c1cec6946248f087fc80f4596c49ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.037941540167992324, 'precision': 0.9450171821305842, 'recall': 0.9526558891454965, 'f1_score': 0.9488211615871189}\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.ner import NERModel\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "transformers_logger = logging.getLogger('transformers')\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# We use the bert base cased pre-trained model.\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = NERModel('bert', 'bert-base-chinese', args=train_args)\n",
    "\n",
    "# Train the model, there is no development or validation set for this dataset\n",
    "# https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping\n",
    "model.train_model(train_df)\n",
    "\n",
    "# Evaluate the model in terms of accuracy score\n",
    "result, model_outputs, preds_list = model.eval_model(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T15:16:36.300032Z",
     "end_time": "2023-04-26T16:53:05.931397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "869194f16b5540409dbfcbaf9656d4c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e21617a04d34c50b7e703eeb1ae576f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "{'曾': 'O'}\n",
      "{'被': 'O'}\n",
      "{'譽': 'O'}\n",
      "{'為': 'O'}\n",
      "{'牛': 'O'}\n",
      "{'樟': 'O'}\n",
      "{'芝': 'O'}\n",
      "{'大': 'O'}\n",
      "{'王': 'O'}\n",
      "{'的': 'O'}\n",
      "{'劉': 'B-PER'}\n",
      "{'威': 'I-PER'}\n",
      "{'甫': 'I-PER'}\n",
      "{'，': 'O'}\n",
      "{'2': 'O'}\n",
      "{'0': 'O'}\n",
      "{'1': 'O'}\n",
      "{'6': 'O'}\n",
      "{'年': 'O'}\n",
      "{'起': 'O'}\n",
      "{'用': 'O'}\n",
      "{'直': 'O'}\n",
      "{'銷': 'O'}\n",
      "{'手': 'O'}\n",
      "{'法': 'O'}\n",
      "{'，': 'O'}\n",
      "{'招': 'O'}\n",
      "{'攬': 'O'}\n",
      "{'會': 'O'}\n",
      "{'員': 'O'}\n",
      "{'投': 'O'}\n",
      "{'資': 'O'}\n",
      "{'培': 'O'}\n",
      "{'植': 'O'}\n",
      "{'牛': 'O'}\n",
      "{'樟': 'O'}\n",
      "{'芝': 'O'}\n",
      "{'，': 'O'}\n",
      "{'誆': 'O'}\n",
      "{'稱': 'O'}\n",
      "{'每': 'O'}\n",
      "{'單': 'O'}\n",
      "{'位': 'O'}\n",
      "{'保': 'O'}\n",
      "{'證': 'O'}\n",
      "{'金': 'O'}\n",
      "{'7': 'O'}\n",
      "{'萬': 'O'}\n",
      "{'5': 'O'}\n",
      "{'元': 'O'}\n",
      "{'，': 'O'}\n",
      "{'期': 'O'}\n",
      "{'滿': 'O'}\n",
      "{'還': 'O'}\n",
      "{'可': 'O'}\n",
      "{'全': 'O'}\n",
      "{'額': 'O'}\n",
      "{'領': 'O'}\n",
      "{'回': 'O'}\n",
      "{'保': 'O'}\n",
      "{'證': 'O'}\n",
      "{'金': 'O'}\n"
     ]
    }
   ],
   "source": [
    "strs = \"\"\"曾被譽為牛樟芝大王的劉威甫，2016年起用直銷手法，招攬會員投資培植牛樟芝，誆稱每單位保證金7萬5元，期滿還可全額領回保證金，涉嫌吸金30億，檢方訊問後，董事長劉威甫100萬交保，台灣分公司總經理莊立平則是諭令200萬交保，遭限制出境。平頭、白髮，遭檢調人員帶回北檢，他是被封為牛樟芝大王的台商劉威甫，2014年在中國風光成立中國珍菌堂集團成了董事長，如今卻是檢調人員頭號鎖定對象。集團總經理莊立平和公司掛名負責人張桂銘也都到北檢複訊，就是因為他們宣稱以「椴木培植法」培育牛樟芝，甚至還能直接拿商品買賣，利用直銷方式吸引會員。珍菌堂廣告：「建設有牛樟樹，種苗培育基地 。」集團經理莊立平在大陸認識劉威甫後，說服董事長在2016年回台開設分公司，還誆稱牛樟芝能治癌症，做噱頭，吸引3萬人成為會員，更誆稱期滿後本金可全數退回，短短兩年涉嫌吸金30億。記者吳欣倫：「珍菌堂他賣的不只是成品，還主打可以種植牛樟芝，讓大家成為小農，也就是成為養菌培植戶，只要每一平方公尺，就能來培植而且可以形成所謂的互利互助概念，更主打說只要投資花7萬5千元，甚至你只要每周付7百元，你就可以拿回所謂的保證金。」珍菌堂董事長劉威甫：「我們投入一些開發資金也好，都是公司這個行業前所未見。」但2018年開始，台灣投資人陸續收不到紅利，莊立平似乎怕東窗事發，把共享獎金設為浮動制 ，看牛樟液銷量多少才依比例發放，投資人氣得提告。大樓保全：「搬很久了，好久了 至少有5年了。」而總經理莊立平遭指控是全案主謀，他全盤否認 ，只說自己是珍菌堂海外經銷商，最後被檢方諭令200萬交保並遭限制出境，董事長劉威甫則供稱，所有買賣合約都是總經理負責，與他無關，最後被諭令百萬交保。●東森新聞關心您\n",
    "不良行為，請勿模仿（封面圖／東森新聞）\"\"\"\n",
    "samples = [' '.join(strs)]\n",
    "predictions, _ = model.predict(samples)\n",
    "for idx, sample in enumerate(samples):\n",
    "  print('{}: '.format(idx))\n",
    "  for word in predictions[idx]:\n",
    "    print('{}'.format(word))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T18:05:35.216127Z",
     "end_time": "2023-04-26T18:05:40.705041Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
